{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/company.png\" style=\"width:504;height:228;align:middle;\">\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center><h1>Brian Carter</h1>\n",
    " \n",
    "<br>\n",
    " \n",
    "<h4>brianthomascarter@gmail.com</h4>\n",
    " \n",
    "<br>\n",
    " \n",
    "<h3> Part of Speech Tagging Review </h3><br>\n",
    "\n",
    "A light (not mathematical, definetly not comprehensive) <br>review of some experiences using part of speech tagging libraries in Python.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Outline of Talk</h1>\n",
    "\n",
    "<ul style=\"font-size:200%;\">\n",
    "  <li>Introduce POS</li>\n",
    "  <li>Motivation</li>\n",
    "  <li>Experiences with NLTK</li>\n",
    "  <li>Available NLP Libraries</li>\n",
    "    <ul>\n",
    "    <li>Installation</li>\n",
    "    <li>Speed Comparison</li>\n",
    "    <li>Tag Differences</li>\n",
    "    </ul>\n",
    "  <li>Summary</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing POS\n",
    "\n",
    "- NLP is a huge discipline!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - Parsing: \n",
    "        - Unstructured text into structured (useful) data.\n",
    "        - Simple as counting words (tfidf)\n",
    "        - **The sentence is made up of a noun, verb, object (POS)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - Classificaiton:\n",
    "        - Entity Recognition (place name, person's name)\n",
    "        - Sentiment Analysis <br>\n",
    "        - Prepocessing data for scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - Production:\n",
    "        - Chatbots\n",
    "        - Speech to Text\n",
    "\n",
    "<font size=\"1.5\">Sources <br> https://worldwritable.com/natural-language-processing-for-programmers-c21a4aff3cb9#.5gwmli2uq </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- POS attempts to assign each word in a sentence **a tag**. \n",
    "- High level tags are nouns, verbs adjectives.. \n",
    "- There are many more categories and sub-categories. \n",
    "- The grammatical structure can be use to help  **another application reason about it** the text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h4> <font color=\"blue\"> They ate the pizza with anchovies </font> </h4>\n",
    " \n",
    " <img src=\"images/spacy1.png\" alt=\"parse1\" style=\"width:600px;height:260px;align:middle;\"/>\n",
    " <img src=\"images/spacy2.png\" alt=\"parse2\" style=\"width:500px;height:200px;align:middle;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/penn-tree-semi.png\" alt=\"penn\"  style=\"width:504;height:328;align:middle;\"/>\n",
    " \n",
    " <font size=\"1.5\">Sources <br> http://www.slideshare.net/isabelleaugenstein/information-extraction-with-linked-data   </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why POS?\n",
    "\n",
    "*Another application reason about it*\n",
    "\n",
    "- There is some NLP functionality in [*scikit learn*]( http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "    - bag of words, ngrams, tf matrix, tf-idf \n",
    "\n",
    "- Using POS can extend\n",
    "    - bag of words of nouns only etc.\n",
    "    - noun phrases <span style=\"background-color: #faf0ee\"> (<u>The election-year <b>politics</b></u> are annoying for <u>many <b>people</b></u>)</span>\n",
    "    - Entities (people, places) extraction. \n",
    "\n",
    "<h4> Topic Modelling </h4>\n",
    "\n",
    "- **Secret Recipe for Topic Modeling Themes** ...employ Part-of-Speech tagging or “POS-Tagging” in order to identify and ultimately “stop out” all of the words that are not nouns!this is a good way to capture thematic information; it certainly does not capture such things as affect (i.e. attitudes towards the theme)... \n",
    "\n",
    "\n",
    "<font size=\"1.5\">Sources <br> https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/ <br> http://www.matthewjockers.net/2013/04/12/secret-recipe-for-topic-modeling-themes/\n",
    "    </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## My Motivation\n",
    "\n",
    "I am general Data Scientist. \n",
    "\n",
    "- Topic model for free text survey data  ( 2014 using nltk ) <br><br>\n",
    "\n",
    "- Word Cloud for pillreports.com project ( 2015 using nltk / TextBlob  ) <br>\n",
    "    - [Text Mining and Visualization: Case Studies Using Open-Source Tools](https://www.crcpress.com/Text-Mining-and-Visualization-Case-Studies-Using-Open-Source-Tools/Hofmann-Chisholm/p/book/9781482237573)\n",
    "    - [Code for chapter](https://github.com/iBrianCarter/pillreports_python)<br><br>\n",
    "    \n",
    " \n",
    "- Home Depot Kaggle competition (Mid 2016 using Spacy.io)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "clusterDF['Tagged']=(clusterDF['User Report']).map(lambda x: pos_tag(word_tokenize(x)))\n",
    "clusterDF['Rich_words']=(clusterDF['Tagged']).map(lambda x: ' '.join([y[0].lower() for y in x if y[1] in ['JJ','NN']]))\n",
    "\n",
    "words1 = ' '.join(clusterDF['Rich_words'][(clusterDF['SC Category']=='mdxx')])\n",
    "words2 = ' '.join(clusterDF['Rich_words'][(clusterDF['SC Category']=='amphet')])\n",
    "words3 = ' '.join(clusterDF['Rich_words'][(clusterDF['SC Category']=='unknown')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " <img src=\"images/wordcloud.png\" alt=\"wordcloud\"  style=\"width:450;height:400;align:middle;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    " ## [Kaggle Home Depot Competiton](https://www.kaggle.com/c/home-depot-product-search-relevance/data)\n",
    " \n",
    " - Predict the accuracy of search results\n",
    " - Human raters evaluate the impact of changes to the Home Depot search algorithms.\n",
    " - Relevance (1 not relevant) to 3(highly relevant)<br>\n",
    " - **Instructions: Examine the; search query, product image , product title against the Product Description. Focus on the following attributes when comparing the product to the query - Brand, Material, and Functionality.**\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path_to_data = \".\\\\parser-comparions-pycon-dublin-2016\\\\kaggle_data\\\\\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "hdTrain = pd.read_csv(path_to_data + \"train.csv\",encoding=\"ISO-8859-1\")\n",
    "hdDesc =  pd.read_csv(path_to_data + \"product_descriptions.csv\",encoding=\"UTF-8\")\n",
    "\n",
    "#Add some lenghts\n",
    "from nltk import word_tokenize\n",
    "hdDesc['string_len_PD'] =  hdDesc['product_description'].apply(lambda x: len(x))\n",
    "hdDesc['word_len_PD'] = hdDesc['product_description'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "hdDesc['word_cum_sum'] = hdDesc['word_len_PD'].cumsum()\n",
    "\n",
    "trainID = hdTrain['id'].sample(n=3)\n",
    "productUID = hdTrain[hdTrain['id'].isin(trainID)]['product_uid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>3700</td>\n",
       "      <td>100650</td>\n",
       "      <td>Glacier Bay 2-piece Dual Flush Elongated Toile...</td>\n",
       "      <td>handicap toilet</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43306</th>\n",
       "      <td>131701</td>\n",
       "      <td>146279</td>\n",
       "      <td>Philips 90W Equivalent Bright White (3000K) PA...</td>\n",
       "      <td>lights outdoor bulbs</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53236</th>\n",
       "      <td>161556</td>\n",
       "      <td>162739</td>\n",
       "      <td>The Hillman Group Visual Impact 10 in. x 14 in...</td>\n",
       "      <td>construction plastic</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  product_uid                                      product_title  \\\n",
       "1186     3700       100650  Glacier Bay 2-piece Dual Flush Elongated Toile...   \n",
       "43306  131701       146279  Philips 90W Equivalent Bright White (3000K) PA...   \n",
       "53236  161556       162739  The Hillman Group Visual Impact 10 in. x 14 in...   \n",
       "\n",
       "                search_term  relevance  \n",
       "1186        handicap toilet       1.67  \n",
       "43306  lights outdoor bulbs       2.33  \n",
       "53236  construction plastic       2.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdTrain[hdTrain['id'].isin(trainID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_description</th>\n",
       "      <th>string_len_PD</th>\n",
       "      <th>word_len_PD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>100650</td>\n",
       "      <td>The Glacier Bay 2-piece Elongated Toilet in Wh...</td>\n",
       "      <td>793</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46278</th>\n",
       "      <td>146279</td>\n",
       "      <td>Create an inviting atmosphere with the Philips...</td>\n",
       "      <td>1290</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62738</th>\n",
       "      <td>162739</td>\n",
       "      <td>The Hillman Group Visual Impact 10 in. x 14 in...</td>\n",
       "      <td>422</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_uid                                product_description  \\\n",
       "649         100650  The Glacier Bay 2-piece Elongated Toilet in Wh...   \n",
       "46278       146279  Create an inviting atmosphere with the Philips...   \n",
       "62738       162739  The Hillman Group Visual Impact 10 in. x 14 in...   \n",
       "\n",
       "       string_len_PD  word_len_PD  \n",
       "649              793          120  \n",
       "46278           1290          234  \n",
       "62738            422           69  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdDesc[hdDesc['product_uid'].isin(productUID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "hdDesc['word_cum_sum'] = hdDesc['word_len_PD'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_description</th>\n",
       "      <th>word_cum_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100011</td>\n",
       "      <td>Recycler 22 in. Personal Pace Variable Speed S...</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100101</td>\n",
       "      <td>PG-21 Lacquer-Resistant Performance Grade Mask...</td>\n",
       "      <td>15957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>101001</td>\n",
       "      <td>ProFlex No-Dig Landscape Edging is an innovati...</td>\n",
       "      <td>152391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124427</th>\n",
       "      <td>224428</td>\n",
       "      <td>The Bosch quick change bi-metal hole saws feat...</td>\n",
       "      <td>17579555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_uid                                product_description  \\\n",
       "10           100011  Recycler 22 in. Personal Pace Variable Speed S...   \n",
       "100          100101  PG-21 Lacquer-Resistant Performance Grade Mask...   \n",
       "1000         101001  ProFlex No-Dig Landscape Edging is an innovati...   \n",
       "124427       224428  The Bosch quick change bi-metal hole saws feat...   \n",
       "\n",
       "        word_cum_sum  \n",
       "10              1945  \n",
       "100            15957  \n",
       "1000          152391  \n",
       "124427      17579555  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdDesc.iloc[[10,100,1000,len(hdDesc)-1],[0,1,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiences\n",
    "\n",
    "- There are a number of NLP libraries available for use in python. \n",
    "- They differ in their intent (reserach vs. application vs. preprocessing for use in model)\n",
    "- They differ in their functionality ( sentence segmentation, word tokenization, entity recognition)\n",
    "- They differ in their output <br>\n",
    "\n",
    "\n",
    "## NLTK\n",
    "\n",
    "- NLKT is go to for gathering and classify unstructure texts. \n",
    "- Often use NLTK for preprocessing and tokenisation, and then feed into a vector representation for machine learning.\n",
    "- For a POS-tagger, lemmatizer, dependeny-analyzer, sentiment analyis, there are here\n",
    "- **But** can be slow, have to do multiple parses of the data for all fucntionality. \n",
    "- Not very well optimized - involving NLTK libraries often means to accept a huge performance loss. \n",
    "- There are newer alternatives.  \n",
    "\n",
    " <font size=\"1.5\">Sources <br> https://www.quora.com/When-is-better-to-use-NLTK-vs-Sklearn-vs-Gensim <br> http://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html <br> https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "<table style=\"font-size:90%;\">\n",
    "    <tr>\n",
    "        <td><b>Library</b></td>\n",
    "        <td><b>Info</b></td>\n",
    "        <td><b>Language</b></td>\n",
    "        <td><b>License</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td><a href = \"http://www.nltk.org/book/ch05.html\">nltk</a></td>\n",
    "       <td><p style=\"font-size:80%;\">Since version 3.1 the  <font color=\"blue\"> perceptron tagger</font> is the  default POS tagger. Written by Matthew Honnibal and Long Duong (NLTK port)</p></td>\n",
    "       <td>Python</td>\n",
    "       <td>Apache 2.0 License</td>\n",
    "   </tr>     \n",
    "   <tr>\n",
    "      <td><a href=\"https://spacy.io\">spacy</a></td>\n",
    "      <td><p style =\"font-size:80%;\">Dependency parser using <font color=\"blue\">greedy average perceptron model</font> to train</p></td>\n",
    "      <td>Cython with Python API</td>\n",
    "      <td>MIT</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "       <td><a href=\"http://www.clips.ua.ac.be/pattern\">pattern</a></td>\n",
    "       <td><p style =\"font-size:80%;\"> Multi-purpose package for web mining, NLP, machine\n",
    "learning and network analysis. Focus on ease-of-use. Regular expressions-based shallow parser for English (identifies sentence constituents, e.g., nouns, verbs), using a finite state part-of-speech tagger <font color = \"red\">(Brill Tagger, 1992, rule based)</font></p></td>\n",
    "        <td>Python</td>\n",
    "        <td>BSD</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href = \"https://textblob.readthedocs.io/en/dev/#\">textblob</a></td>\n",
    "      <td><p style = \"font-size:80%;\">Currently has two POS tagger implementations, located in textblob.taggers. The default is the <font color = \"red\">PatternTagger</font> which uses the same implementation as the Pattern library.\n",
    "The second implementation is NLTKTagger which uses NLTK‘s TreeBank tagger. Numpy is required to use the\n",
    "NLTKTagger.</p></td>\n",
    "       <td>Python</td>\n",
    "       <td>MIT</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><a href = \"http://nlp.stanford.edu/software/lex-parser.shtml\">stanford</a></td>\n",
    "        <td><p style = \"font-size:80%;\">Version 3.6.0, 2015-12-09. Java implementation of the <font color = \"green\">conditional log-linear part-of-speech taggers</font> described in Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network <p></td>\n",
    "        <td>Java with nltk API</td>\n",
    "        <td>GNU General Public License V2)</td>\n",
    "    </tr>\n",
    "  </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Out of Scope\n",
    "\n",
    "- Have played around with \n",
    "    - [Stanford CoreNLP](http://stanfordnlp.github.io/CoreNLP/index.html)\n",
    "    - [Syntaxnet aka. Parsey McParseface](https://github.com/tensorflow/models/tree/master/syntaxnet)\n",
    "\n",
    "\n",
    "<img src=\"images/command-prompt.png\" alt=\"command\" style=\"width:400;height:250;align:middle;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "[nltk](http://www.nltk.org/data.html)\n",
    "1. Included in Anaconda distro or **pip install -U nlkt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Spacy](https://spacy.io/docs/usage/)\n",
    "1. **pip install -U spacy**\n",
    "2. **python - m spacy.en download all** - english <br>\n",
    "    \n",
    "[pattern]( http://textminingonline.com/getting-started-with-pattern) <br>\n",
    "1. **pip install pattern**\n",
    "\n",
    "    \n",
    "[TextBlob](  https://textblob.readthedocs.io/en/dev/#get-it-now )   \n",
    "1. **pip install -U textblob**\n",
    "2. **python -m textblob.download_corpora** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "[Stanford Parser](http://nlp.stanford.edu/software/tagger.html#History)\n",
    "- Download the Stanford POS Tagger from http://nlp.stanford.edu/software/tagger.html and the Stanford NER Tagger  from http://nlp.stanford.edu/software/CRF-NER.html \n",
    "- Unzip the files and place in folder. <br> <br>\n",
    "\n",
    "- <p style=\"color:red;\"> Note the NLKT and Stanford Tools APIs/tools change quite often. This will work for NLTK 3.2 and Stanford 3.6.0 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Check the java environment. #Need the latest version of Java 1.8 running\n",
    "# Windows:(for %i in (java.exe) do @echo.   %~$PATH:i)\n",
    "# Mac/Unix (which java)\n",
    "\n",
    "import os\n",
    "java_path = \"C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath\\\\java.exe\"\n",
    "os.environ['JAVA_HOME'] = java_path\n",
    "\n",
    "##Set paths to model and jar file\n",
    "stanford_dir = '.\\\\Desktop\\\\parser-comparions-pycon-dublin-2016\\\\stanford\\\\stanford-postagger-2015-12-09\\\\'\n",
    "modelfile = stanford_dir + 'models\\\\english-bidirectional-distsim.tagger'\n",
    "jarfile = stanford_dir + 'stanford-postagger.jar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running the Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#NLKT part of speech tagger and word tokenizer\n",
    "from nltk import pos_tag as nltkPOS\n",
    "from nltk import word_tokenize as wt\n",
    "#Spacy \n",
    "from spacy.en import English\n",
    "spacyPOS = English()\n",
    "#Pattern\n",
    "from pattern.en import tag as patternPOS\n",
    "#Textblob\n",
    "from textblob import TextBlob as textBlobPOS\n",
    "#Stanford Parser\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "stanPOS = StanfordPOSTagger(model_filename=modelfile, path_to_jar=jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Mary', 'NNP'), (u'had', 'VBD'), (u'a', 'DT'), (u'little', 'JJ'), (u'lamb', 'NN'), (u'.', '.')]\n",
      "[(u'Mary', u'NNP'), (u'had', u'VBD'), (u'a', u'DT'), (u'little', u'JJ'), (u'lamb', u'NN'), (u'.', u'.')]\n",
      "[(u'Mary', u'NNP'), (u'had', u'VBD'), (u'a', u'DT'), (u'little', u'JJ'), (u'lamb', u'NN'), (u'.', u'.')]\n",
      "[(u'Mary', u'NNP'), (u'had', u'VBD'), (u'a', u'DT'), (u'little', u'JJ'), (u'lamb', u'NN')]\n",
      "[(u'Mary', u'NNP'), (u'had', u'VBD'), (u'a', u'DT'), (u'little', u'JJ'), (u'lamb', u'NN'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "testText = u\"Mary had a little lamb.\"\n",
    "print( nltkPOS(wt(testText)) ) #Have to tokenize the sentence. What about tokenize sentences? \n",
    "print( [(word.orth_,word.tag_) for word in spacyPOS(testText)] ) #Note spacy returns multiple tags not just POS\n",
    "print ( patternPOS(testText) )\n",
    "print ( textBlobPOS(testText).tags ) #TextBlob similar to Spacy returns multiple tags\n",
    "print ( stanPOS.tag(wt(testText)) ) #Have to tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "resultsPOS = []\n",
    "\n",
    "\n",
    "for sampleSize in [10,100,1000,10000]:\n",
    "    for repeatTest in range(0,30):\n",
    "        \n",
    "        sampleDesc = hdDesc.sample(n=sampleSize)\n",
    "        totalWords = sum(sampleDesc['word_len_PD'])\n",
    "        \n",
    "        #Move through and collect times\n",
    "        start_time = time.time()\n",
    "        parseText = sampleDesc['product_description'].apply(lambda x: nltkPOS(wt(x)))\n",
    "        parseTextNLKT = parseText\n",
    "        time_nltkPOS = (time.time() - start_time)\n",
    "        punc_nltkPOS = sum(parseText.map(lambda x: len([y[0] for y in x if y[1] in ['.',',',':']])))\n",
    "      \n",
    "        start_time = time.time()\n",
    "        parseText = sampleDesc['product_description'].apply(lambda x: [(word.orth_,word.tag_) for word in spacyPOS(x)])\n",
    "        time_spacyPOS = (time.time() - start_time)\n",
    "        punc_spacyPOS = sum(parseText.map(lambda x: len([y[0] for y in x if y[1] in ['.',',',':']])))\n",
    "              \n",
    "        start_time = time.time()\n",
    "        parseText =  sampleDesc['product_description'].apply(lambda x: patternPOS(x))\n",
    "        time_patternPOS = (time.time() - start_time)\n",
    "        punc_patternPOS = sum(parseText.map(lambda x: len([y[0] for y in x if y[1] in ['.',',',':']])))\n",
    "               \n",
    "        start_time = time.time()\n",
    "        parseText = sampleDesc['product_description'].apply(lambda x: textBlobPOS(x).tags)\n",
    "        parseTextBlob = parseText\n",
    "        time_textBlobPOS = (time.time() - start_time)\n",
    "        punc_textBlobPOS = sum(parseText.map(lambda x: len([y[0] for y in x if y[1] in ['.',',',':']])))\n",
    "            \n",
    "        if sampleSize == 10:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            parseText = sampleDesc['product_description'].apply(lambda x: stanPOS.tag(wt(x)))\n",
    "            time_stanPOS = (time.time() - start_time)\n",
    "            punc_stanPOS = sum(parseText.map(lambda x: len([y[0] for y in x if y[1] in ['.',',',':','(',')']])))\n",
    "        else:\n",
    "            time_stanPOS = np.nan\n",
    "            punc_stanPOS = np.nan\n",
    "        \n",
    "        #Save the train scores in RESULTS\n",
    "        results_dict = {'sampleSize' : sampleSize,\n",
    "                        'totalWords': totalWords ,\n",
    "                        'nlkt.time' : time_nltkPOS,\n",
    "                        'spacy.time' : time_spacyPOS,\n",
    "                        'pattern.time' : time_patternPOS,\n",
    "                        'textblob.time': time_textBlobPOS,\n",
    "                        'stan.time' : time_stanPOS,\n",
    "                        'nlkt.puncs' : punc_nltkPOS,\n",
    "                        'spacy.puncs' :  punc_spacyPOS,\n",
    "                        'pattern.puncs' : punc_patternPOS,\n",
    "                        'textblob.puncs': punc_textBlobPOS,\n",
    "                        'stan.puncs' : punc_stanPOS\n",
    "                        }\n",
    "                        \n",
    "        resultsPOS.append(dict(results_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "colOrder = ['sampleSize','totalWords','nlkt.time','spacy.time','pattern.time','textblob.time','stan.time','nlkt.puncs','spacy.puncs','pattern.puncs',\n",
    " 'textblob.puncs','stan.puncs'] \n",
    "\n",
    "resultsPOS = pd.DataFrame(resultsPOS)\n",
    "resultsPOS = resultsPOS[colOrder]\n",
    "resultsPOS.to_csv(\"posTimes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speed Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " <img src=\"images/speed1.png\" alt=\"speed1\"  width=\"800\" height=\"600\" align=\"center\"/>\n",
    " \n",
    " <font size=\"1.5\">Note: Plots created with ggplot2 in R  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"images/speed2.png\" alt=\"speed2\"  width=\"800\" height=\"600\" align=\"center\"/>\n",
    " \n",
    " <font size=\"1.5\">Note: Plots created with ggplot2 in R  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"images/speed3.png\" alt=\"speed3\"  width=\"800\" height=\"600\" align=\"center\"/>\n",
    " \n",
    " <font size=\"1.5\">Note: Plots created with ggplot2 in R  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "- Accuracy is subjective!! pos taggers are trained on human annoted text. *The Wall Street Journal*\n",
    "- What about agreement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " <img src=\"images/unicorn1.png\" alt=\"uni1\"  style=\"width:150px;height:120px;align:middle;\"/>\n",
    " <img src=\"images/unicorn2.png\" alt=\"uni2\"  style=\"width:250;height:125;align:middle;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "unicornMaskReview = u'As a unicorn myself, I am quite satisfied with the authenticity of this particular item. I believe that for far too long, other species have had a rather undisciplined view of how we truly are. This mask gives such individuals the chance to \"walk a mile in our shoes\" per say. Obviously there are several unattractive flaws with this particular mask, such as how the mouth is continuously gaping as if to say something or take a small bite of food. However, all other educational aspects seem fairly correct except for the fact that I myself have a black coat of fur (yes, we black unicorns exist) and I would like to see a more diverse line of products for the following fiscal year. All in all I thoroughly enjoy the chance for humans to truly understand us for what we are.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a unicorn myself, I am quite satisfied with the authenticity of this particular item. I believe that for far too long, other species have had a rather undisciplined view of how we truly are. This mask gives such individuals the chance to \"walk a mile in our shoes\" per say. Obviously there are several unattractive flaws with this particular mask, such as how the mouth is continuously gaping as if to say something or take a small bite of food. However, all other educational aspects seem fairly correct except for the fact that I myself have a black coat of fur (yes, we black unicorns exist) and I would like to see a more diverse line of products for the following fiscal year. All in all I thoroughly enjoy the chance for humans to truly understand us for what we are.\n"
     ]
    }
   ],
   "source": [
    "print(unicornMaskReview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Convert list of tuples to data.frame\n",
    "def tuplesToDF (listOfTuples,parserName):\n",
    "    words = pd.Series([x[0] for x in listOfTuples])\n",
    "    pos   = pd.Series([x[1] for x in listOfTuples])\n",
    "    \n",
    "    # wordPOSDF = pd.DataFrame()\n",
    "    wordPosDF = pd.DataFrame({'words': words.values, parserName: pos.values})\n",
    "    #wordPosDF = pd.DataFrame(words.values, pos.values)\n",
    "    return wordPosDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Parse the data\n",
    "umrNLTK = nltkPOS(wt(unicornMaskReview))\n",
    "umrSpacy = [(word.orth_,word.tag_) for word in spacyPOS(unicornMaskReview)]\n",
    "umrPattern  = patternPOS(unicornMaskReview)\n",
    "umrTextBlob =  textBlobPOS(unicornMaskReview).tags \n",
    "umrStan =   stanPOS.tag(wt(unicornMaskReview))\n",
    "\n",
    "#Create DataFrame and merge on words\n",
    "dfNLTK = tuplesToDF(umrNLTK,\"posNLTK\")\n",
    "dfSpacy = tuplesToDF(umrSpacy,\"posSpacy\")\n",
    "dfPattern = tuplesToDF(umrPattern,\"posPattern\")\n",
    "dfTextBlob = tuplesToDF(umrTextBlob,\"posTextBlob\")\n",
    "dfStan = tuplesToDF(umrStan,\"posStan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Concat all the df except dfTextBlob - different shape\n",
    "fullPOS = pd.concat([dfNLTK, dfSpacy['posSpacy']], axis=1)\n",
    "fullPOS = pd.concat([fullPOS,dfPattern['posPattern']],axis=1)\n",
    "fullPOS = pd.concat([fullPOS,dfStan['posStan']],axis=1)\n",
    "\n",
    "colOrder = ['words','posNLTK','posSpacy','posPattern','posStan']\n",
    "fullPOS = fullPOS[colOrder]\n",
    "\n",
    "#Create Flag where descrepency\n",
    "def all_same(a,b,c,d):\n",
    "    items = [a,b,c,d]\n",
    "    return all(x == items[0] for x in items)\n",
    "\n",
    "fullPOS['agreement'] = fullPOS.apply(lambda row: all_same(row['posNLTK'],\n",
    "                                                          row['posSpacy'],\n",
    "                                                          row['posPattern'],\n",
    "                                                          row['posStan']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>posNLTK</th>\n",
       "      <th>posSpacy</th>\n",
       "      <th>posPattern</th>\n",
       "      <th>posStan</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unicorn</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>myself</td>\n",
       "      <td>NN</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>long</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>had</td>\n",
       "      <td>VBD</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VBD</td>\n",
       "      <td>VBN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>``</td>\n",
       "      <td>``</td>\n",
       "      <td>``</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>\"</td>\n",
       "      <td>''</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>say</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>VB</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>gaping</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VBG</td>\n",
       "      <td>JJ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>-LRB-</td>\n",
       "      <td>(</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>-RRB-</td>\n",
       "      <td>)</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>following</td>\n",
       "      <td>JJ</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VBG</td>\n",
       "      <td>JJ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>RB</td>\n",
       "      <td>IN</td>\n",
       "      <td>IN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>truly</td>\n",
       "      <td>VB</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>understand</td>\n",
       "      <td>JJ</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words posNLTK posSpacy posPattern posStan agreement\n",
       "2       unicorn      JJ       NN         NN      NN     False\n",
       "3        myself      NN      PRP        PRP     PRP     False\n",
       "23         long      RB       RB         JJ      JJ     False\n",
       "28          had     VBD      VBN        VBD     VBN     False\n",
       "47           ``      ``       ``          \"      ``     False\n",
       "54           ''      ''       ''          \"      ''     False\n",
       "56          say      NN       NN         VB      NN     False\n",
       "76       gaping     VBG      VBG        VBG      JJ     False\n",
       "112           (       (    -LRB-          (      NN     False\n",
       "119           )       )    -RRB-          )      NN     False\n",
       "134   following      JJ      VBG        VBG      JJ     False\n",
       "139          in      IN       RB         IN      IN     False\n",
       "143       enjoy     VBP      VBP         VB     VBP     False\n",
       "149       truly      VB       RB         RB      RB     False\n",
       "150  understand      JJ       VB         VB      VB     False"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullPOS[fullPOS['agreement']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "<table style=\"font-size:70%;\">\n",
    "    <tr>\n",
    "        <td><b>Library</b></td>\n",
    "        <td><b>Speed</b></td>\n",
    "        <td><b>Reported Accuracy <p style=\"font-size:70%;\">on Penn Treebank</p></b></td>\n",
    "        <td><b>Functionality on single parse</b></td>\n",
    "         <td><b>Languages</b></td>\n",
    "         <td><b>Last Update <p style=\"font-size:70%;\">GIT commit</p></b></td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "       <td><a href = \"http://www.nltk.org/book/ch05.html\">nltk</a></td>\n",
    "       <td>1</td>\n",
    "       <td>90.0%</td>\n",
    "       <td><p style=\"font-size:80%;\">APIs to multiple other libraries.Returns pos tag only with parse.</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Multiple depending on API</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Oct-16</p></td>\n",
    "    </tr>\n",
    "    \n",
    "        \n",
    "    <tr>\n",
    "       <td><a href=\"https://spacy.io\">spacy</a></td>\n",
    "       <td>0.82</td>\n",
    "       <td>91.8%</td>\n",
    "       <td><p style=\"font-size:80%;\">tokens, sentences, entity type, lemma, url, number, email, out-of-vocab, part-of-speech, syntactic dependency, probablity, sentiment, noun phrase\n",
    "</p></td>\n",
    "       <td><p style=\"font-size:80%;\">en,de</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Nov-16</p></td>\n",
    "    </tr>\n",
    "    \n",
    "     <tr>\n",
    "       <td><a href=\"http://www.clips.ua.ac.be/pattern\">pattern</a></td>\n",
    "       <td>1.13</td>\n",
    "       <td>91.0%</td>\n",
    "       <td><p style=\"font-size:80%;\">tag, chunk, role, noun phrase</p></td>\n",
    "       <td><p style=\"font-size:80%;\">en, de, fr, es, nl, it</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Jan-15</p></td>\n",
    "    </tr>\n",
    "    \n",
    "      <tr>\n",
    "       <td><a href = \"https://textblob.readthedocs.io/en/dev/#\">textblob</a></td>\n",
    "       <td>1.07</td>\n",
    "       <td></td>\n",
    "       <td><p style=\"font-size:80%;\">pos, noun phrases, sentiment</p></td>\n",
    "       <td><p style=\"font-size:80%;\">en, de, fr</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Jun-14</p></td>\n",
    "    </tr>\n",
    "    \n",
    "       \n",
    "      <tr>\n",
    "       <td><a href = \"http://nlp.stanford.edu/software/lex-parser.shtml\">stanford</a></td>\n",
    "       <td>256</td>\n",
    "       <td>91.7%</td>\n",
    "       <td><p style=\"font-size:80%;\">same as NLTK API</p></td>\n",
    "       <td><p style=\"font-size:80%;\">en, de, arabic, chinese</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Dec-15</p></td>\n",
    "    </tr>\n",
    "    \n",
    "         \n",
    "      <tr>\n",
    "       <td><a href = \"http://stanfordnlp.github.io/CoreNLP/\">coreNLP</a></td>\n",
    "       <td></td>\n",
    "       <td>89.6%</td>\n",
    "       <td><p style=\"font-size:80%;\"></p></td>\n",
    "       <td><p style=\"font-size:80%;\">en, de, fr, es, arabic, chinese</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Nov-16</p></td>\n",
    "    </tr>\n",
    "    \n",
    "      <tr>\n",
    "       <td><a href =\"https://github.com/tensorflow/models/tree/master/syntaxnet\">SyntaxNet</a></td>\n",
    "       <td></td>\n",
    "       <td>94.0%</td>\n",
    "       <td><p style=\"font-size:80%;\"></p></td>\n",
    "       <td><p style=\"font-size:80%;\">40+ languages</p></td>\n",
    "       <td><p style=\"font-size:80%;\">Nov-16</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"6\"><p style=\"font-size:100%;\"> There are Cloud Provides providing POS. <a href=\"https://www.microsoft.com/cognitive-services/en-us/Linguistic-Analysis-API/documentation/POS-tagging\">Microsoft Liguistics Analysis API</a> - Free ,\n",
    "<a href=\"https://cloud.google.com/natural-language/docs/basics\">Google Natural Language API Basics </a> - Free (Beta) </p></td>\n",
    "    </tr>\n",
    "   </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/thankyou.png\" alt=\"command\"  width=\"600\" height=\"400\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
